# CS125-Research
Rough draft of abstract:

We provide a model, analysis and results from an experiment implementing a daily lecture feedback requirement in a large-lecture environment introductory computer science course. Gathering feedback early and often has been demonstrated to be effective at monitoring student progress and aids in identifying stumbling points in material and presentation throughout a course. We present results from an experiment taking this idea to an extreme. Daily feedback on our lectures (provided by 600 students per semester) lets us keep a finger on the pulse of the course, immediately detecting topics that are unsettling to students and permitting rapid movement through material in which students express mastery. Our lecture feedback app is conveniently available on the web and in iOS and Android app formats. Daily lecture feedback takes just seconds for students to submit with these tools. Each day, from these feedback apps, we gather student ID and a partner ID, a lecture productivity rating (0-10) and a list of topics understood and any needing additional attention from each student. Students are motivated to provide feedback because providing it: 1) contributes a small amount toward their course grade, 2) allows them to alter their learning environment by sitting in different locations and working on lecture activities with a different student each lecture session, and 3) means they will work with a variety of students of varying skill levels - experiencing both mentor and apprentice roles throughout the duration of the course. While participating in the lecture feedback process has obvious benefits for the students and the course it also provides us with a rich dataset we use to mine for answers to important questions related to large-lecture course outcomes. We cross-reference the contributed lecture productivity ratings with student roster and performance information such as the studentâ€™s major, college, year-in-school, gender, grades on weekly quizzes, and cumulative semester grades. From this data we attempt to identify relationships and provide answers to questions such as: Do students perform better on weekly quizzes when they work the prior week with students at higher, lower or about the same skill level as themselves? Do students claim their lecture sessions are more productive when they work with students at a higher, lower or about the same skill level? Do students of one gender report they have more productive lecture sessions with someone of the same or opposite gender? Does performance on subsequent quizzes validate these opinions, or oppose them. Do students who work with a large variety of student partners from a larger set of skill levels perform better or worse in the course overall than peers who primarily work with those of similar skill levels? We provide a summary of these and other research questions in this work.
